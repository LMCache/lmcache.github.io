---
layout: post
title: "CacheGen: Storing your KV cache to disk and AWS S3 while loading blazingly fast!"
thumbnail-img: /assets/img/cachegen.png
share-img: /assets/img/cachegen.png
author: Kuntai Du
image: /assets/img/cachegen.png
---

**TL;DR:** üöÄ CacheGen allows you to quickly load KV caches from disk or even from AWS S3 storage! It compresses your KV cache 3x smaller compared to quantization, while still allow you generating high-quality responses. Stop recomputing‚Äîget faster first-token times and smoother LLM serving at cloud scale!

---

## Why CacheGen?

Modern LLMs rely on long contexts, but reprocessing those every time is slow and wastes compute. 
Existing LLM engines like vLLM already supports caching these contexts (in the form of KV cache) in GPU memory (and with LMCache, in CPU memory).
But for your popular chatting applications and agentic applications, even GPU and CPU memory altogether may not be enough, but loading KV caches from storage devices like disk and AWS S3 is slow -- even slower than just recomputing the KV caches.
**CacheGen** lets you **store** the KV cache to remote storage (S3, disk, etc.), and **load it back ‚Äî- way faster than recomputing from text**. Perfect for remembering the valuable contexts for all your users and agents.

---

## Key Results üìä


| System                | Mean TTFT (ms) | Mean TPOT (ms) |
|-----------------------|:--------------:|:--------------:|
| **LMCache + CacheGen**|   **737**      |    **47.7**    | 
| Naive vLLM            |   4,355        |     247.6      |
| Fireworks             |   2,353        |     664.7      |
| DeepInfra             |   2,949        |      79.0      |
| Baseten               | 113,239        |     174.9      |


Takeaway: **CacheGen cuts Time-To-First-Token (TTFT) by up to 3√ó compared to other baselines!**

---

## How Does It Work?

- **Compress:** CacheGen encodes and compresses the KV cache (using residue coding and custom quantization).
- **Decompress:** High-performance CUDA kernel to quickly decompress KV caches.

---

## Quick Start üõ†Ô∏è

```bash
uv pip install vllm
uv pip install lmcache

# Start cache server
lmcache_server localhost 65434

# Start vLLM+LMCache servers (example config below)
LMCACHE_CONFIG_FILE=example.yaml CUDA_VISIBLE_DEVICES=2 vllm serve meta-llama/Llama-3.1-8B-Instruct --gpu-memory-utilization 0.8 --port 8020 --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
```

example.yaml
```yaml
chunk_size: 2048
local_cpu: False
remote_url: "lm://localhost:65434"
remote_serde: "cachegen"
```

## Contact

- **LMCache Github: [https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache)**
- **Chat with the Developers** **[Interest Form](https://forms.gle/mQfQDUXbKfp2St1z7)**
- **LMCache [slack](https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ)**
- **vLLM Production-Stack [channel](https://vllm-dev.slack.com/archives/C089SMEAKRA)**


**CacheGen: persistent, streaming context for fast, scalable LLMs‚Äîthe LMCache Lab way!** üöÄ