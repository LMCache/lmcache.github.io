---
layout: post
title: "üéâ LMCache Hits 5,000+ GitHub Stars ‚Äî Thank You, Community!"
subtitle: "A milestone that shows KV cache has become a first-class citizen in the LLM inference stack"
tags: [milestone, community, github, stars]
comments: true
author: LMCache Team
---

We're thrilled to share that LMCache has officially crossed 5,000 GitHub stars! üöÄ This milestone is not just a number ‚Äî it's a strong signal that KV cache technology has become a first-class citizen in the LLM inference stack, and that our community is leading the way.

## What is LMCache?

LMCache is the first open-source KV cache library designed to turbocharge LLM inference efficiency. It provides a production-ready foundation for manipulating, storing, and reusing KV cache at scale.

Key capabilities include:

**KV cache CPU offloading** ‚Üí reuse across multiple queries, not tied to a single inference run.

**Cross-engine sharing** ‚Üí unlocks interoperability across inference engines.

**Inter-GPU KV transfer** ‚Üí enabling pipeline and PD disaggregation.

**Experimental features** ‚Üí compression, blending (for RAG & agent workflows), and smarter cache mining to maximize hit rates.

LMCache focuses almost exclusively on real production deployments, where efficiency and scale matter most.

We're also taking the next step by proposing LMCache to the PyTorch Foundation, ensuring the project continues to grow as a community-driven standard.

## The Journey to 5K Stars

When LMCache launched in June 2024, KV cache was barely discussed outside research papers. Few predicted that KV cache would become central to improving throughput and reducing inference latency across the industry.

Fast forward one year, and the momentum is clear:

- KV cache has become a standard technique for scaling LLM inference.
- LMCache has been adopted in production by many companies to cut costs and latency.
- The project has become a dependency for other OSS efforts like Dynamo, llm-d, and vLLM production stack, further boosting adoption.

üìà You can view the star growth here: [https://www.star-history.com/#LMCache/lmcache](https://www.star-history.com/#LMCache/lmcache)

![LMCache Star History](/assets/img/star-history-2025827.png)
*LMCache's GitHub star growth over time*

## Community Power

LMCache originated as a research prototype developed at the University of Chicago. Since then, it has evolved into a thriving open-source ecosystem thanks to contributions from a diverse global community, including engineers from IBM, NVIDIA, Red Hat, Microsoft, Redis, Cohere, AMD, Tencent, Weka, ByteDance, Pliops, and researchers from Stanford, MIT, UC Berkeley, CMU, ‚Ä¶and many more.

This broad collaboration proves the project's value across both academia and industry.

## Looking Ahead

The 5K star milestone is just the beginning. We're already working on:

- Expanding LMCache integrations across cloud and OSS inference stacks.
- Evolving KV cache persistence and disaggregation into standard building blocks for LLM infra.
- Continuing the process of donating LMCache into a larger open-source foundation to secure its long-term future.

## How You Can Get Involved

We'd love for you to join us on the next phase of LMCache's journey:

- ‚≠ê [Star the project on GitHub](https://github.com/LMCache/lmcache)
- üí¨ [Join our Slack community](https://join.slack.com/t/lmcache/shared_invite/zt-2nvly6s6c-7br9jvFzlc2lPoxmQzmP2g)
- üìù [Fill out our interest form](https://lmcache.ai/register)
- üìö [Read the documentation](https://lmcache.readthedocs.io/)
- üõ† [Pick up a good first issue](https://github.com/LMCache/lmcache/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)

## üí° Final Note

5,000 stars in just over a year is a community achievement. It shows that we all bet on the right idea ‚Äî that KV cache isn't a side-optimization, but a core building block for the future of LLM infrastructure.

Thank you for building this future with us. Here's to the next 5,000! üöÄ
